{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Fasthugs fastai LM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wps01VMMDcfP"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubensmau/nlp/blob/master/Fasthugs_fastai_LM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR1CR8TXDcd9",
        "colab_type": "text"
      },
      "source": [
        "# FastHugs  --- USING roberta-base model\n",
        "\n",
        "Modified from https://github.com/morganmcg1/fasthugs .  \n",
        "My goal is to use a trained roberta language model for  masked language prediction / next word prediction in a fastai environment.\n",
        "I want to import a trained model in a large corpus, use as it is in fastai2, also as language mode. And make predictions using learn.predict.\n",
        "In the last step,I would like to modify the model head  to make classification of other task. Since I am learning fastai, I assume it would be easier to have all the customization done in fastai environment\n",
        "\n",
        "This notebook was run from Google Colab\n",
        "\n",
        "This notebook gives a full run through to fine-tune a text classification model with **HuggingFace ü§ó transformers** and the new **fastai-v2** library.\n",
        "\n",
        "## Things You Might Like (‚ù§Ô∏è ?)\n",
        "**FastHugsTokenizer:** A tokenizer wrapper than can be used with fastai-v2's tokenizer.\n",
        "\n",
        "**FastHugsModel:** A model wrapper over the HF models, more or less the same to the wrapper's from HF fastai-v1 articles mentioned below\n",
        "\n",
        "**Vocab:** A function to extract the vocab depending on the pre-trained transformer (HF hasn't standardised this processes üò¢).\n",
        "\n",
        "**Padding:** Padding settings for the padding token index and on whether the transformer prefers left or right padding\n",
        "\n",
        "**Vocab for Albert-base-v2**: .json for Albert-base-v2's vocab, otherwise this has to be extracted from a SentencePiece model file, which isn't fun\n",
        "\n",
        "**Model Splitters:** Functions to split the classification head from the model backbone in line with fastai-v2's new definition of `Learner`\n",
        "\n",
        "## Housekeeping\n",
        "### Pretrained Transformers only for now üòê\n",
        "Initially, this notebook will only deal with finetuning HuggingFace's pretrained models. It covers BERT, DistilBERT, RoBERTa and ALBERT pretrained classification models only. These are the core transformer model architectures where HuggingFace have added a classification head. HuggingFace also has other versions of these model architectures such as the core model architecture and language model model architectures.\n",
        "\n",
        "If you'd like to try train a model from scratch HuggingFace just recently published an article on [How to train a new language model from scratch using Transformers and Tokenizers](https://huggingface.co/blog/how-to-train). Its well worth reading to see how their `tokenizers` library can be used, independent of their pretrained transformer models.\n",
        "\n",
        "### Read these first üëá\n",
        "This notebooks heavily borrows from [this notebook](https://www.kaggle.com/melissarajaram/roberta-fastai-huggingface-transformers) , which in turn is based off of this [tutorial](https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta) and accompanying [article](https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2). Huge thanks to  Melissa Rajaram and Maximilien Roberti for these great resources, if you're not familiar with the HuggingFace library please given them a read first as they are quite comprehensive.\n",
        "\n",
        "### fastai-v2  ‚úåÔ∏è2Ô∏è‚É£\n",
        "[This paper](https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/) introduces the v2 version of the fastai library and you can follow and contribute to v2's progress [on the forums](https://forums.fast.ai/). This notebook uses the small IMDB dataset and is based off the [fastai-v2 ULMFiT tutorial](http://dev.fast.ai/tutorial.ulmfit). Huge thanks to Jeremy, Sylvain, Rachel and the fastai community for making this library what it is. I'm super excited about the additinal flexibility v2 brings. üéâ\n",
        "\n",
        "### Dependencies üì•\n",
        "If you haven't already, install HuggingFace's `transformers` library with: `pip install transformers`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRD1wNUVDcd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hide\n",
        "# CUDA ERROR DEBUGGING\n",
        "# https://lernapparat.de/debug-device-assert/\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nIXURMcHDsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip freeze | grep fastprogress\n",
        "#!pip uninstall fastprogress"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnph2FqTDtuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q fastai2==0.0.13     #### it seems that using this version , I get fewer errors\n",
        "!pip install -q transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPXKcK1PDceF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hide\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from fastai2.basics import *\n",
        "from fastai2.text.all import *\n",
        "from fastai2.callback.all import *\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaForMaskedLM,  RobertaTokenizer, RobertaConfig\n",
        "from transformers import AlbertForSequenceClassification, AlbertTokenizer, AlbertConfig,AutoModelWithLMHead,AutoTokenizer\n",
        "\n",
        "\n",
        "!pip install -q transformers\n",
        "from __future__ import print_function\n",
        "import ipywidgets as widgets\n",
        "from transformers import pipeline\n",
        "\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t8b9yuODceJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hide\n",
        "path = untar_data(URLs.IMDB_SAMPLE)\n",
        "model_path = Path('models')\n",
        "df = pd.read_csv(path/'texts.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZHx6VK1DceO",
        "colab_type": "text"
      },
      "source": [
        "## FastHugs Tokenizer\n",
        "This tokenizer wrapper is initialised with the pretrained HF tokenizer, you can also specify the max_seq_len if you want longer/shorter sequences. Given text it returns tokens and adds separator tokens depending on the model type being used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlJHvzLfDceO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FastHugsTokenizer():\n",
        "    \"\"\" \n",
        "        transformer_tokenizer : takes the tokenizer that has been loaded from the tokenizer class\n",
        "        model_type : model type set by the user\n",
        "        max_len : override default sequence length, typically 512 for bert-like models\n",
        "    \"\"\"\n",
        "    def __init__(self, transformer_tokenizer, model_type = 'roberta', max_seq_len=None, **kwargs): \n",
        "        self.tok = transformer_tokenizer\n",
        "        self.max_seq_len = ifnone(max_seq_len, self.tok.max_len)\n",
        "        self.model_type = model_type\n",
        "        self.pad_token_id = self.tok.pad_token_id\n",
        "        \n",
        "    def do_tokenize(self, t:str):\n",
        "        \"\"\"Limits the maximum sequence length and add the special tokens\"\"\"\n",
        "        CLS = self.tok.cls_token\n",
        "        SEP = self.tok.sep_token\n",
        "#         import pdb\n",
        "#         pdb.set_trace()\n",
        "        #print(t)\n",
        "        if 'roberta' in model_type:\n",
        "            tokens = self.tok.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n",
        "        else:\n",
        "            tokens = self.tok.tokenize(t)[:self.max_seq_len - 2]\n",
        "        #print(tokens)\n",
        "        return [CLS] + tokens + [SEP]\n",
        "\n",
        "    def __call__(self, items): \n",
        "        for t in items: yield self.do_tokenize(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUGagfW_BV4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #### Tokenizer test\n",
        "# t = fasthugstok()\n",
        "# t.do_tokenize('i am tall')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-8k8wjyDceT",
        "colab_type": "text"
      },
      "source": [
        "## FastHugs Model\n",
        "This `nn.module` wraps the pretrained transformer model, initialises it with is config file. If you'd like to make configuration changes to the model, you can do so in this class. The `forward` of this module is taken straight from Melissa's notebook above and its purpose is to create the attention mask and grab only the logits from the output of the model (as the HF transformer models also output the loss)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "i9ryHSHPDceU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# More or less copy-paste from https://www.kaggle.com/melissarajaram/roberta-fastai-huggingface-transformers/data\n",
        "class FastHugsModelLM(nn.Module):\n",
        "    def __init__(self, pretrained_model_name, model_class, config_class, max_seq_len=None):\n",
        "        super(FastHugsModelLM, self).__init__()\n",
        "        self.config = config_class.from_pretrained(pretrained_model_name)\n",
        "        #self.config.num_labels = n_class  #NO CLASSIFICATION TASK\n",
        "        if max_seq_len is not None: self.config.max_position_embeddings = max_seq_len  ### era max_len antes, acho que tava errado\n",
        "        \n",
        "        self.transformer = model_class.from_pretrained(pretrained_model_name, config = self.config, \n",
        "                                    cache_dir=model_path/f'{pretrained_model_name}')\n",
        "\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        attention_mask = (input_ids!=1).type(input_ids.type()) \n",
        "        logits = self.transformer(input_ids, attention_mask = attention_mask)[0] \n",
        "        return logits\n",
        "\n",
        "    def reset(self): pass  # self.h.zero_()    ##### LEARNER COMPLAINED AT ONE POINT THAT RESET ATTRIBUTE WAS MISSING"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKgnuHmkeiqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dir(FastHugsModelLM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz_SxKI2DceY",
        "colab_type": "text"
      },
      "source": [
        "## Padding\n",
        "Pass the initialised transformer tokenizer to set the index for the padding token and the side padding should be applied; e.g. BERT, Roberta prefers padding to the right, so we set `pad_first=False`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1kC3yHADceZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transformer_padding(transformer_tokenizer): \n",
        "    if transformer_tokenizer.padding_side == 'right': \n",
        "        pad_first=False\n",
        "    return partial(pad_input_chunk, pad_first=pad_first, pad_idx=transformer_tokenizer.pad_token_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12HX39wSDcef",
        "colab_type": "text"
      },
      "source": [
        "## Lets get training\n",
        "### Select our HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A_U15NmExk3",
        "colab_type": "text"
      },
      "source": [
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "import torch\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "outputs = model(input_ids, labels=labels)\n",
        "loss, logits = outputs[:2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkWSJLkDDceg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models_dict = {'bert_classification': (BertForSequenceClassification, BertTokenizer, BertConfig, \n",
        "                                       'bert-base-uncased', 'bert_class_splitter'),\n",
        "                'roberta_classification': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig, \n",
        "                                           'roberta-base', 'roberta_clas_splitter'),\n",
        "                'distilbert_classification': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig, \n",
        "                                              'distilbert-base-uncased', 'distilbert_clas_splitter'),\n",
        "                'albert_classification': (AlbertForSequenceClassification, AlbertTokenizer, AlbertConfig, \n",
        "                                          'albert-base-v2', 'albert_clas_splitter'),\n",
        "                'roberta-maskedlm' :     (RobertaForMaskedLM,RobertaTokenizer,RobertaConfig,         # ALTERNTATIVES AutoModelWithLMHead,AutoTokenizer\n",
        "                                           \"roberta-base\", \"roberta_base_splitter\")\n",
        "                \n",
        "\n",
        "              }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIpV9fIfDcek",
        "colab_type": "text"
      },
      "source": [
        "Grab the model, tokenizer and config that we'd like to use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLnURL_eDcel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_type = 'roberta-maskedlm'\n",
        "model_class, tokenizer_class, config_class, pretrained_model_name, tfmr_splitter = models_dict[model_type]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETc1B6vxDceq",
        "colab_type": "text"
      },
      "source": [
        "We can also change the max sequence length for the tokenizer and transformer here. If its not set it will default to the pretrained model's default, typically `512`. 1024 or even 2048 can also be used depending on your GPU memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbaSPlPADcer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_len = 288  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pNL2ywVDcev",
        "colab_type": "text"
      },
      "source": [
        "## Geting HuggingFace Tokenizer into fastai-v2\n",
        "Intialise the tokenizer needed for the pretrained model, this will download the `vocab.json` and `merges.txt` files needed. Specifying `cache_dir` will allow us easily access them, otherwise they will be saved to a Torch cache folder here `~/.cache/torch/transformers`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZN0nCxpDcew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name, \n",
        "                                                        cache_dir=model_path/f'{pretrained_model_name}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXon9ElwDce2",
        "colab_type": "text"
      },
      "source": [
        "**Create fasthugstok function:** Lets incorporate the `transformer_tokenizer` into fastai-v2's framework by specifying a fucntion to pass to `Tokenizer.from_df`. (Note `from_df` is the only method I have tested)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9y5-Pu_xDce3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fasthugstok = partial(FastHugsTokenizer, transformer_tokenizer = transformer_tokenizer, \n",
        "                      model_type=model_type, max_seq_len=max_seq_len)  ## None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl5DpgOrDce7",
        "colab_type": "text"
      },
      "source": [
        "**Set up fastai-v2's Tokenizer.from_df:** We pass `rules=[]` to override fastai's default text processing rules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Tf59vMDce7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_fn = Tokenizer.from_df(text_cols='text', res_col_name='text', tok_func=fasthugstok, rules=[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6R_HCvoDce_",
        "colab_type": "text"
      },
      "source": [
        " ## Vocab\n",
        " Model and vocab files will be saved with files names as a long string of digits and letters (e.g. `d9fc1956a0....f4cfdb5feda.json` generated from the etag from the AWS S3 bucket as described [here in the HuggingFace repo](https://github.com/huggingface/transformers/issues/2157). For readability I prefer to save the files in a specified directory and model name so that it can be easily found and accessed in future.\n",
        " \n",
        "(Note: To avoid saving these files twice you could look at the `from_pretrained` and `cached_path` functions in HuggingFace's `PreTrainedTokenizer` class definition to find the code that downloads the files and maybe modify them to download directly to your specified directory withe desired name. I haven't had time to go that deep.)\n",
        "\n",
        "Load vocab file into a `list` as expected by fastai-v2. The HF pretrained tokenizer vocabs come in different file formats depending on the tokenizer you're using; BERT's vocab is saved as a .txt file, RoBERTa's is saved as a .json and Albert's has to be extracted from a SentencePiece model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5_v5NiTDcfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vocab(transformer_tokenizer, pretrained_model_name):\n",
        "    if pretrained_model_name in ['bert-base-uncased', 'distilbert-base-uncased',]:\n",
        "        transformer_vocab = list(transformer_tokenizer.vocab.keys())\n",
        "    else:\n",
        "        transformer_tokenizer.save_vocabulary(model_path/f'{pretrained_model_name}')\n",
        "        suff = 'json'\n",
        "        if pretrained_model_name in ['albert-base-v2']:\n",
        "            with open(model_path/f'{pretrained_model_name}/alberta_v2_vocab.{suff}', 'r') as f: \n",
        "                transformer_vocab = json.load(f) \n",
        "        else:\n",
        "            with open(model_path/f'{pretrained_model_name}/vocab.{suff}', 'r') as f: \n",
        "                transformer_vocab = list(json.load(f).keys()) \n",
        "    return transformer_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVMCWyeVDcfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer_vocab = get_vocab(transformer_tokenizer, pretrained_model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK8xwGcHDcfH",
        "colab_type": "text"
      },
      "source": [
        "## Setup Data\n",
        "### Create Dataset\n",
        "Lets add our custom tokenizer function (`tok_fn`) and `transformer_vocab` here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MKLrLKGKDcfI",
        "colab_type": "code",
        "outputId": "c9a422f8-8a6a-4785-e851-d399ffc77198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "splits = ColSplitter()(df)\n",
        "x_tfms = [attrgetter(\"text\"), tok_fn, Numericalize(vocab=transformer_vocab)]\n",
        "dsets = Datasets(df, splits=splits, tfms=[x_tfms], dl_type=SortedDL)   ### tfms=[x_tfms] ,[attrgetter(\"label\"), Categorize()]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wps01VMMDcfP",
        "colab_type": "text"
      },
      "source": [
        "### Dataloaders\n",
        "Here we use our `transformer_padding()` wrapper when loading the dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC5NztDfDcfW",
        "colab_type": "text"
      },
      "source": [
        "### (Alternatively) Factory dataloader\n",
        "Here we set:\n",
        "- `tok_tfm=tok_fn` to use our HF tokenizer\n",
        "- `text_vocab=transformer_vocab` to load our pretrained vocab\n",
        "- `before_batch=transformer_padding(transformer_tokenizer)` to use our custom padding function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoNzun5oDcfX",
        "colab_type": "code",
        "outputId": "cc4b3cdc-c2c6-4238-aafc-1e92b1643994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "# Factory\n",
        "bs =4\n",
        "dls = TextDataLoaders.from_df(df, text_col=\"text\", tok_tfm=tok_fn, text_vocab=transformer_vocab,\n",
        "                              before_batch=transformer_padding(transformer_tokenizer),\n",
        "                              is_lm=True, valid_col='is_valid', bs=bs,device='cuda')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvSl5_99Dcfb",
        "colab_type": "code",
        "outputId": "81920906-a734-41b4-8568-0c0670fda922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "dls.show_batch(max_n=2, trunc_at=60)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;s&gt; ƒ†This ƒ†guy ƒ†has ƒ†no ƒ†idea ƒ†of ƒ†cinema . ƒ†Okay , ƒ†it ƒ†seems ƒ†he ƒ†made ƒ†a ƒ†few ƒ†interest ig ƒ†theater ƒ†shows ƒ†in ƒ†his ƒ†youth , ƒ†and ƒ†about ƒ†two ƒ†acceptable ƒ†movies ƒ†that ƒ†had ƒ†success ƒ†more ƒ†of ƒ†political ƒ†reasons ƒ†cause ƒ†they ƒ†tricked ƒ†the ƒ†communist ƒ†censorship . ƒ†This ƒ†all ƒ†is ƒ†very ƒ†good , ƒ†but ƒ†look ƒ†carefully : ƒ†HE ƒ†DOES ƒ†NOT ƒ†KNOW ƒ†HIS ƒ†J</td>\n",
              "      <td>ƒ†This ƒ†guy ƒ†has ƒ†no ƒ†idea ƒ†of ƒ†cinema . ƒ†Okay , ƒ†it ƒ†seems ƒ†he ƒ†made ƒ†a ƒ†few ƒ†interest ig ƒ†theater ƒ†shows ƒ†in ƒ†his ƒ†youth , ƒ†and ƒ†about ƒ†two ƒ†acceptable ƒ†movies ƒ†that ƒ†had ƒ†success ƒ†more ƒ†of ƒ†political ƒ†reasons ƒ†cause ƒ†they ƒ†tricked ƒ†the ƒ†communist ƒ†censorship . ƒ†This ƒ†all ƒ†is ƒ†very ƒ†good , ƒ†but ƒ†look ƒ†carefully : ƒ†HE ƒ†DOES ƒ†NOT ƒ†KNOW ƒ†HIS ƒ†J OB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ƒ†Knox ville ƒ†both ƒ†give ƒ†below ƒ†average ƒ†performances . ƒ†The ƒ†latter ƒ†was ƒ†pretty ƒ†good ƒ†as ƒ†St if ler ƒ†but ƒ†he ƒ†tries ƒ†way ƒ†too ƒ†hard ƒ†here . ƒ†The ƒ†latter ƒ†just ƒ†seems ƒ†to ƒ†be ƒ†looking ƒ†for ƒ†a ƒ†paycheck ƒ†and ƒ†nothing ƒ†else . ƒ†Jessica ƒ†Simpson ƒ†isn 't ƒ†known ƒ†for ƒ†her ƒ†acting ƒ†nor ƒ†is ƒ†she ƒ†really ƒ†known ƒ†for ƒ†her ƒ†singing . ƒ†She &lt;/s&gt; &lt;s&gt; ƒ†In</td>\n",
              "      <td>ville ƒ†both ƒ†give ƒ†below ƒ†average ƒ†performances . ƒ†The ƒ†latter ƒ†was ƒ†pretty ƒ†good ƒ†as ƒ†St if ler ƒ†but ƒ†he ƒ†tries ƒ†way ƒ†too ƒ†hard ƒ†here . ƒ†The ƒ†latter ƒ†just ƒ†seems ƒ†to ƒ†be ƒ†looking ƒ†for ƒ†a ƒ†paycheck ƒ†and ƒ†nothing ƒ†else . ƒ†Jessica ƒ†Simpson ƒ†isn 't ƒ†known ƒ†for ƒ†her ƒ†acting ƒ†nor ƒ†is ƒ†she ƒ†really ƒ†known ƒ†for ƒ†her ƒ†singing . ƒ†She &lt;/s&gt; &lt;s&gt; ƒ†In ƒ†the</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_0QEGsvDcfe",
        "colab_type": "text"
      },
      "source": [
        "## Model Splitters\n",
        "HuggingFace's models with names such as: `RobertaForSequenceClassification` are core transformer models with a classification head. Lets split the classification head from the core transformer backbone to enable us use progressive unfreezing and differential learning rates.\n",
        "\n",
        "You can split the model into 3 groups by modifying the splitter function like so:\n",
        "\n",
        "`\n",
        "def roberta_clas_splitter(m):\n",
        "    \"Split the classifier head from the backbone\"\n",
        "    groups = [nn.Sequential(m.transformer.roberta.embeddings,\n",
        "                  m.transformer.roberta.encoder.layer[0],\n",
        "                  m.transformer.roberta.encoder.layer[1],\n",
        "                  m.transformer.roberta.encoder.layer[2],\n",
        "                  m.transformer.roberta.encoder.layer[3],\n",
        "                  m.transformer.roberta.encoder.layer[4],\n",
        "                  m.transformer.roberta.encoder.layer[5],\n",
        "                  m.transformer.roberta.encoder.layer[6],\n",
        "                  m.transformer.roberta.encoder.layer[7],\n",
        "                  m.transformer.roberta.encoder.layer[8])]\n",
        "    groups+= [nn.Sequential(m.transformer.roberta.encoder.layer[9],\n",
        "                  m.transformer.roberta.encoder.layer[10],\n",
        "                  m.transformer.roberta.encoder.layer[11],\n",
        "                  m.transformer.roberta.pooler)]\n",
        "    groups = L(groups + [m.transformer.classifier])\n",
        "    return groups.map(params)\n",
        "`\n",
        "\n",
        "**Classification Head Differences**\n",
        "\n",
        "Interestingly, BERT's classification head is different to RoBERTa's\n",
        "\n",
        "BERT + ALBERT:\n",
        "\n",
        "`\n",
        "(dropout): Dropout(p=0.1, inplace=False)\n",
        "(classifier): Linear(in_features=768, out_features=2, bias=True)\n",
        "`\n",
        "\n",
        "DistilBERT's has a \"pre-classifier\" layer:\n",
        "\n",
        "`\n",
        "(pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
        "(classifier): Linear(in_features=768, out_features=2, bias=True)\n",
        "(dropout): Dropout(p=0.2, inplace=False)`\n",
        "\n",
        "RoBERTa's:\n",
        "\n",
        "`(classifier): RobertaClassificationHead(\n",
        "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "    (dropout): Dropout(p=0.1, inplace=False)\n",
        "    (out_proj): Linear(in_features=768, out_features=2, bias=True))`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0a6E2_EDcff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bert_clas_splitter(m):\n",
        "    \"Split the classifier head from the backbone\"\n",
        "    groups = [nn.Sequential(m.transformer.bert.embeddings,\n",
        "                m.transformer.bert.encoder.layer[0],\n",
        "                m.transformer.bert.encoder.layer[1],\n",
        "                m.transformer.bert.encoder.layer[2],\n",
        "                m.transformer.bert.encoder.layer[3],\n",
        "                m.transformer.bert.encoder.layer[4],\n",
        "                m.transformer.bert.encoder.layer[5],\n",
        "                m.transformer.bert.encoder.layer[6],\n",
        "                m.transformer.bert.encoder.layer[7],\n",
        "                m.transformer.bert.encoder.layer[8],\n",
        "                m.transformer.bert.encoder.layer[9],\n",
        "                m.transformer.bert.encoder.layer[10],\n",
        "                m.transformer.bert.encoder.layer[11],\n",
        "                m.transformer.bert.pooler)]\n",
        "    groups = L(groups + [m.transformer.classifier]) \n",
        "    return groups.map(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWNURbHBDcfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def albert_clas_splitter(m):\n",
        "    groups = [nn.Sequential(m.transformer.albert.embeddings,\n",
        "                m.transformer.albert.encoder.embedding_hidden_mapping_in, \n",
        "                m.transformer.albert.encoder.albert_layer_groups,\n",
        "                m.transformer.albert.pooler)]\n",
        "    groups = L(groups + [m.transformer.classifier]) \n",
        "    return groups.map(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFLVGcbWDcfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def distilbert_clas_splitter(m):\n",
        "    groups = [nn.Sequential(m.transformer.distilbert.embeddings,\n",
        "                m.transformer.distilbert.transformer.layer[0], \n",
        "                m.transformer.distilbert.transformer.layer[1],\n",
        "                m.transformer.distilbert.transformer.layer[2],\n",
        "                m.transformer.distilbert.transformer.layer[3],\n",
        "                m.transformer.distilbert.transformer.layer[4],\n",
        "                m.transformer.distilbert.transformer.layer[5],\n",
        "                m.transformer.pre_classifier)]\n",
        "    groups = L(groups + [m.transformer.classifier]) \n",
        "    return groups.map(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA-9U5RSDcfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def roberta_clas_splitter(m):\n",
        "    \"Split the classifier head from the backbone\"\n",
        "    groups = [nn.Sequential(m.transformer.roberta.embeddings,\n",
        "                  m.transformer.roberta.encoder.layer[0],\n",
        "                  m.transformer.roberta.encoder.layer[1],\n",
        "                  m.transformer.roberta.encoder.layer[2],\n",
        "                  m.transformer.roberta.encoder.layer[3],\n",
        "                  m.transformer.roberta.encoder.layer[4],\n",
        "                  m.transformer.roberta.encoder.layer[5],\n",
        "                  m.transformer.roberta.encoder.layer[6],\n",
        "                  m.transformer.roberta.encoder.layer[7],\n",
        "                  m.transformer.roberta.encoder.layer[8],\n",
        "                  m.transformer.roberta.encoder.layer[9],\n",
        "                  m.transformer.roberta.encoder.layer[10],\n",
        "                  m.transformer.roberta.encoder.layer[11],\n",
        "                  m.transformer.roberta.pooler)]\n",
        "    groups = L(groups + [m.transformer.classifier])\n",
        "    return groups.map(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSCP6FsUQUfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def roberta_base_splitter(m):                       ## ADDED SPLITTER\n",
        "    \"Split the classifier head from the backbone\"\n",
        "    groups = [nn.Sequential(m.transformer.roberta.embeddings,\n",
        "         m.transformer.roberta.encoder.layer[0],\n",
        "                  m.transformer.roberta.encoder.layer[1],\n",
        "                  m.transformer.roberta.encoder.layer[2],\n",
        "                  m.transformer.roberta.encoder.layer[3],\n",
        "                  m.transformer.roberta.encoder.layer[4],\n",
        "                  m.transformer.roberta.encoder.layer[5],\n",
        "                  m.transformer.roberta.encoder.layer[6],\n",
        "                  m.transformer.roberta.encoder.layer[7],\n",
        "                  m.transformer.roberta.encoder.layer[8],\n",
        "                  m.transformer.roberta.encoder.layer[9],\n",
        "                  m.transformer.roberta.encoder.layer[10],\n",
        "                  m.transformer.roberta.encoder.layer[11],\n",
        "                  m.transformer.roberta.pooler)]\n",
        "    groups = L(groups + [m.transformer.lm_head])      ### SUBSTITUTED CLASSIFIER BY LM_HEAD\n",
        "    return groups.map(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpPJylJbDcfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splitters = {#'bert_clas_splitter':bert_clas_splitter,\n",
        "            #'albert_clas_splitter':albert_clas_splitter,\n",
        "            #'distilbert_clas_splitter':distilbert_clas_splitter,\n",
        "            #'roberta_clas_splitter':roberta_clas_splitter,\n",
        "            'roberta_base_splitter': roberta_base_splitter}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J6CN_FWDcf4",
        "colab_type": "text"
      },
      "source": [
        "### Load Model with configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG4rqWfkDcf6",
        "colab_type": "text"
      },
      "source": [
        "Here we can tweak the HuggingFace model's config file before loading the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvNf0Q5JdPFg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "45d1e659-481b-4d2b-f880-254d420dfb81"
      },
      "source": [
        "model_class, config_class,pretrained_model_name"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(transformers.modeling_auto.AutoModelWithLMHead,\n",
              " transformers.configuration_roberta.RobertaConfig,\n",
              " 'roberta-base')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dVMfKA1_Dcf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fasthugs_model_lm = FastHugsModelLM(model_class=model_class, config_class=config_class,\n",
        "                               pretrained_model_name = pretrained_model_name, \n",
        "                               max_seq_len=None)         #n_class=dsets.c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JTlmrQ_Dcf_",
        "colab_type": "text"
      },
      "source": [
        "Initialise everything our Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-kPxSA5DcgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_func = partial(Adam, decouple_wd=True)\n",
        "\n",
        "cbs = [MixedPrecision(clip=0.1), SaveModelCallback()]\n",
        "\n",
        "loss = CrossEntropyLossFlat() #LabelSmoothingCrossEntropy  \n",
        "\n",
        "splitter = splitters[tfmr_splitter]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgxrA65na6OW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svDloiTZDcgE",
        "colab_type": "text"
      },
      "source": [
        "## Time to train\n",
        "### Create our learner  - OF A LANGUAGE MODEL - TO PREDICT THE NEXT WORD IN A SENTENCE , OR A MASKED WORD, LIKE IN THE ORIGINAL ROBERTA MODEL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1gwA0M1RDcgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#learn = Learner(dls, fasthugs_model_lm, opt_func=opt_func, splitter=splitter, loss_func=loss, cbs=cbs, metrics=[accuracy])\n",
        "#learn = language_model_learner(dls, fasthugs_model_lm, drop_mult=0.3,pretrained=True, metrics=[accuracy, Perplexity()])\n",
        "#learn = TextLearner(dls, fasthugs_model_lm,splitter=splitter,loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
        "learn = LMLearner(dls, fasthugs_model_lm, loss_func=CrossEntropyLossFlat(), splitter=splitter,cbs=cbs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QMuXdsEjg0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decodifica (copia):\n",
        "    return  [ transformer_vocab[copia[i]] for i in range(len(copia))] #copia[i].item(),\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvAm1gVlJJZO",
        "colab_type": "text"
      },
      "source": [
        "#  CHECK SHAPES OF XB, YB AND MODEL(XB) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZWG348RJT_Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36e1cd04-65d9-46c0-f500-4786e9243f10"
      },
      "source": [
        "xb,yb= learn.dls.one_batch()\n",
        "xb.shape, yb.shape"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 72]), torch.Size([4, 72]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6845621f-8cc8-4235-ae7e-3feb3fe6286f",
        "id": "EPzycD7EYztI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "ll = [learn.model(xb)[0][i].argmax().item() for i in range(72) ]\n",
        "decodifica(ll)[:10]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " 'You',\n",
              " 'ƒ†know',\n",
              " 'ƒ†a',\n",
              " 'ƒ†movie',\n",
              " 'ƒ†will',\n",
              " 'ƒ†not',\n",
              " 'ƒ†go',\n",
              " 'ƒ†well',\n",
              " 'ƒ†when']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3ry5UDjUcr4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "7083e2ee-c3a0-4113-96e6-670b573ac1de"
      },
      "source": [
        "decodifica(yb[0])[:10]"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ƒ†You',\n",
              " 'ƒ†know',\n",
              " 'ƒ†a',\n",
              " 'ƒ†movie',\n",
              " 'ƒ†will',\n",
              " 'ƒ†not',\n",
              " 'ƒ†go',\n",
              " 'ƒ†well',\n",
              " 'ƒ†when',\n",
              " 'ƒ†John']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itb5k0_IDcgI",
        "colab_type": "text"
      },
      "source": [
        "### Stage 1 training\n",
        "Lets freeze the model backbone and only train the classifier head. `freeze_to(1)` means that only the classifier head is trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0o7_9_oYDcgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.freeze_to(1)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9sWAH97rDcgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#learn.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBDpQCnlDcgS",
        "colab_type": "text"
      },
      "source": [
        "Lets find a learning rate to train our classifier head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZppfnuluJW3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbwK0eOhJYHj",
        "colab_type": "text"
      },
      "source": [
        "##  ERROR WHEN RUNNING .FIT OR OTHER HIGH LEVEL ROUTINES\n",
        "\n",
        "ValueError: Expected input batch_size (72) to match target batch_size (288)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K7I2ZbKDcgT",
        "colab_type": "code",
        "outputId": "2e6f57e3-15ea-4534-82b0-a1233e447b4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "learn.fit(1)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-8587f3539821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m;\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/learner.py\u001b[0m in \u001b[0;36m_do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m;\u001b[0m                        \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCancelTrainException\u001b[0m\u001b[0;34m:\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/learner.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m                            \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_backward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, targ, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_2d\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1836\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (72) to match target batch_size (288)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J_qGkAkDcgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.recorder.plot_lr_find()\n",
        "plt.vlines(6.918e-05, 0.6, 1.1)\n",
        "plt.vlines(0.04786, 0.6, 1.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9yj6DjZjDcgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.fit_one_cycle(3, lr_max=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zQe5RcLOACe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_PxWRlIDcgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('fasthugs-lm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyucfXxSDcgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.recorder.plot_loss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWR87TaQ4GSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1 = erro"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUJxPe6GDcgs",
        "colab_type": "text"
      },
      "source": [
        "### Stage 2 training\n",
        "And now lets train the full model with differential learning rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MwCeYXFDcgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.unfreeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bAsjazOSDcgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#learn.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmyCkK7NDcg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.lr_find(suggestions=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fZdtM920IHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7qDoqScDcg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.recorder.plot_lr_find()\n",
        "plt.vlines(2.51e-08, 0.2, 0.5)\n",
        "plt.vlines(6.30e-05, 0.2, 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "i4fJeJg5DchD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.fit_one_cycle(3, lr_max=slice(1e-6, 1e-5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRxUn9R9DchH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('roberta-fasthugs-stg2-3e-5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvDsn4Q6fYxx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JZzmrcjDchM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.recorder.plot_loss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAEZFmcxDchS",
        "colab_type": "text"
      },
      "source": [
        "## Lets Look at the model's predictions #### ORIGINAL FASTHUG MODELS OR OUR MODEL WITHOUT LEARN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrYaGFGN7vZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### predicao de fasthugs_model_lm\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
        "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "input_ids = torch.tensor(tokenizer.encode(sequence)).unsqueeze(0)  # Batch size 1\n",
        "copia = input_ids[0].clone()\n",
        "outputs = fasthugs_model_lm(input_ids.cuda())\n",
        "#outputs = model(input)[0]\n",
        "\n",
        "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
        "print(mask_token_index)\n",
        "mask_token_logits = outputs[0, mask_token_index, :]\n",
        "top_5_tokens = torch.topk(mask_token_logits[0], 5, dim=0).indices.tolist()\n",
        "\n",
        "for token in top_5_tokens:\n",
        "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HEit3-SrWM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRw2PvdpQqJG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d5ee8b40-8e4b-4895-d02f-879c65e3a78b"
      },
      "source": [
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")   # AutoTokenizer      \n",
        "model = RobertaForMaskedLM.from_pretrained(\"roberta-base\")   # AutoModelWithLMHead\n",
        "\n",
        "sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
        "\n",
        "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
        "print(tokenizer.mask_token_id)\n",
        "print(mask_token_index,tokenizer.mask_token)\n",
        "token_logits = model(input)[0]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "\n",
        "for token in top_5_tokens:\n",
        "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50264\n",
            "tensor([21]) <mask>\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  reduce our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  lower our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  minimize our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  decrease our carbon footprint.\n",
            "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  cut our carbon footprint.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwHsje98MKGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk6YRZqv5q72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = \"I liked this movie because\"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2\n",
        "learn.get_preds(TEXT)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezbIvE9fDchT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = learn.predict(\"'Distilled models are smaller than the models they mimic. Using them instead of the large\" )\n",
        "#print(res)\n",
        "top5 = torch.topk(res[2][-1], 5, dim=0).indices.tolist()\n",
        "decodifica(top5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-K2qgUIyCK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res[2].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWdoIhdIDchY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai2.interpret import *\n",
        "interp = Interpretation.from_learner(learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR-4JShJDchc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interp.plot_top_losses(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd0yJUZGQZfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### outro exemplo\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\")\n",
        "\n",
        "sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
        "\n",
        "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
        "print(tokenizer.mask_token_id)\n",
        "print(mask_token_index,tokenizer.mask_token)\n",
        "token_logits = model(input)[0]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "\n",
        "for token in top_5_tokens:\n",
        "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOQLWTEx0Orp",
        "colab_type": "text"
      },
      "source": [
        "# CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNriQ-IYSyDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### PARA USAR O DATASET COMPLETO\n",
        "path = untar_data(URLs.IMDB)\n",
        "path.ls()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2Jz56GDMStp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ##### read data from folder\n",
        "# imdb_clas = DataBlock(blocks=(TextBlock.from_folder(path, vocab=dbunch_lm.vocab),CategoryBlock),\n",
        "#                       get_x=read_tokenized_file,\n",
        "#                       get_y = parent_label,\n",
        "#                       get_items=partial(get_text_files, folders=['train', 'test']),\n",
        "#                       splitter=GrandparentSplitter(valid_name='test'))\n",
        "\n",
        "# dls= imdb_clas.dataloaders(path, path=path, bs=bs, seq_len=80)\n",
        "\n",
        "# # ou\n",
        "# #dls_clas = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test', text_vocab=dls_lm.vocab)\n",
        "\n",
        "# learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy,loss_func = CrossEntropyLossFlat(), ).to_fp16()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKpXV5TgZPlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MOfBFPqWH0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### read data from df  - FUNCIONA !!\n",
        "dls = TextDataLoaders.from_df(df, text_col='text', label_col='label', path=path, is_lm=False, valid_col='is_valid', bs=64, seq_len=80)\n",
        "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.3, metrics=[accuracy] ,loss_func= CrossEntropyLossFlat()).to_fp16()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmqcPEIP3Hjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####   read data from df  - FUNCIONA USANDO DATABLOCK\n",
        "imdb_clas = DataBlock(blocks=(TextBlock.from_df('text', is_lm=False),CategoryBlock),\n",
        "                    get_x=attrgetter('text'),\n",
        "                    get_y=ColReader('label'),\n",
        "                    splitter=RandomSplitter())\n",
        "dls = imdb_clas.dataloaders(df)\n",
        "xb,yb = learn.dls.one_batch()\n",
        "xb.shape , yb.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z5M0ZMN6mFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ##### NAO FUNCIONA , ESTE BLOCO-- ESPERAR POR https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0\n",
        "# imdb_clas = DataBlock(blocks=(TextBlock('text', is_lm=False,vocab=dbunch_lm.vocab), CategoryBlock),\n",
        "#                       get_x=attrgetter('text'),\n",
        "#                       get_y=ColReader('label'),\n",
        "#                       splitter=RandomSplitter(),\n",
        "#                       # dl_type=SortedDL, \n",
        "# dls = imdb_clas.dataloaders(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGTqtLPEIow0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xb,yb = learn.dls.one_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv4EkHlGIJ3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xb.shape , yb.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTQ9QpAEMSuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}