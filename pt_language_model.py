# -*- coding: utf-8 -*-
"""PT_Language_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dBSz5TBshpNAjqERZdbCinJVZ2ONGVzi

# General-domain LM pretraining
"""

import json
import pathlib

from fastai.text import *

import numpy as np
import pandas as pd
import html

from sklearn.model_selection import train_test_split

from pathlib import Path

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
root_dir = "/content/gdrive/My Drive/"
#base_dir = root_dir + 'fastai-v3/'

path = Path('/content/gdrive/My Drive/Colab Notebooks/nlp/pt_lm/')

BOS = 'xbos'  # beginning-of-sentence tag
FLD = 'xfld'  # data field tag

#PATH = pathlib.Path("lm/pt/data/")
PATH = pathlib.Path("/mnt/aa/nlp/data/")

#LM_PATH=Path('lm/pt/pt_lm/')
LM_PATH=Path('/mnt/aa/nlp/pt_lm/')
LM_PATH.mkdir(exist_ok=True)

"""## Tokenization"""

LANG_FILENAMES = [str(f) for f in PATH.rglob("*/*")]
print(len(LANG_FILENAMES))
LANG_FILENAMES[0:5]

LANG_TEXT = []
for i in LANG_FILENAMES:
    for line in open(i):
        LANG_TEXT.append(json.loads(line))
        
LANG_TEXT = pd.DataFrame(LANG_TEXT)

LANG_TEXT.head()

###  sort LANG_TEXT by length of text
s = LANG_TEXT.text.str.len().sort_values(ascending=False).index
LANG_TEXT = LANG_TEXT.reindex(s)
LANG_TEXT = LANG_TEXT.reset_index(drop=True)

LANG_TEXT

###  pick only the first 1/21 of the corpus containing the lenghtier "text"
LANG_TEXT = LANG_TEXT[0:LANG_TEXT.shape[0]//21]

LANG_TEXT.to_csv(f"{LM_PATH}/Wiki_PT_Corpus.csv", index=False)

#LANG_TEXT = pd.read_csv(f"{LM_PATH}/Wiki_PT_Corpus.csv")
LANG_TEXT = pd.read_csv(path/'Wiki_PT_Corpus.csv')

(LANG_TEXT.assign(labels = 0)
    .pipe(lambda x: x[['labels', 'text']])
    .to_csv(path/'Wiki_PT_Corpus2.csv', header=None, index=False))

# Getting rid of the title name in the text field
def split_title_from_text(text):
    words = text.split("\n\n")
    if len(words) >= 2:
        return ''.join(words[1:])
    else:
        return ''.join(words)
    
LANG_TEXT['text'] = LANG_TEXT['text'].apply(lambda x: split_title_from_text(x))

LANG_TEXT['text'].apply(lambda x: len(x.split(" "))).sum()

len(set(''.join(LANG_TEXT['text'].values).split(" ")))

re1 = re.compile(r'  +')

def fixup(x):
    x = x.replace('#39;', "'").replace('amp;', '&').replace('#146;', "'").replace(
        'nbsp;', ' ').replace('#36;', '$').replace('\\n', "\n").replace('quot;', "'").replace(
        '<br />', "\n").replace('\\"', '"').replace('<unk>','u_n').replace(' @.@ ','.').replace(
        ' @-@ ','-').replace('\\', ' \\ ')
    return re1.sub(' ', html.unescape(x))

def get_texts(df, n_lbls=1):
    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)
    texts = f'\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)
    for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)
    #texts = texts.apply(fixup).values.astype(str)
    #teste
    texts = texts.apply(fixup).values.astype(str)
    tok = Tokenizer().process_all(partition_by_cores(texts,n_cpus=1))  #, lang='pt'
    #fim teste

    #tok = Tokenizer().proc_all_mp(partition_by_cores(texts)) # splits the list into sublists for processing by each core
    # Lower and upper case is inside the tokenizer
    return tok, list(labels)

def get_all(df, n_lbls):
    tok, labels = [], []
    for i, r in enumerate(df):
        print(i)
        tok_, labels_ = get_texts(r, n_lbls)
        tok += tok_;
        labels += labels_
    return tok, labels

LANG_TEXT = pd.read_csv(path/"Wiki_PT_Corpus2.csv", header=None)#, chunksize=5000)

trn_texts,val_texts = train_test_split(
    LANG_TEXT, test_size=0.1) # split the data into train and validation sets

np.random.seed(42)
trn_idx = np.random.permutation(len(trn_texts)) # generate a random ordering
val_idx = np.random.permutation(len(val_texts))

df_trn = trn_texts.iloc[trn_idx,:] # sort things randomly
df_val = val_texts.iloc[val_idx,:] # sort things randomly

df_trn.columns = ['labels', 'text']
df_val.columns = ['labels', 'text']

df_trn.to_csv(path/'train.csv', header=False, index=False)
df_val.to_csv(path/'test.csv', header=False, index=False) # saving the data in our new format to disk

#chunksize = 10000
chunksize = 5000
df_trn = pd.read_csv(path/'train.csv', header=None, chunksize=chunksize)
df_val = pd.read_csv(path/'test.csv', header=None, chunksize=chunksize)

tok_trn, trn_labels = get_all(df_trn, 1)
tok_val, val_labels = get_all(df_val, 1)

# create a tmp directory to store the upcoming numpy arrays
(path/'tmp').mkdir(exist_ok=True)

# save the train and validation tokens in the tmp directories
np.save(LM_PATH/'tmp'/'tok_trn.npy', tok_trn)
np.save(LM_PATH/'tmp'/'tok_val.npy', tok_val)

"""## Loading dataset"""

tok_trn = np.load(LM_PATH/'tmp'/'tok_trn.npy')
tok_val = np.load(LM_PATH/'tmp'/'tok_val.npy')

# Identify the most common tokens and numericalizing the text
freq = Counter(p for o in tok_trn for p in o) 
freq.most_common(25)

em_sz,nh,nl = 400,1150,3
wd=1e-7
bptt=70
opt_fn = partial(optim.Adam, betas=(0.8, 0.99))

"""### Maximum vocabulary size (60000 tokens), batch size = 32, dropout factor = 0.7, learning rate = 1e-3/2"""

# Truncating our vocab to ignore the rare words
max_vocab = 60000
min_freq = 5

itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq] # getting rid of the rare words
itos.insert(0, '_pad_') # 
itos.insert(0, '_unk_') # itos is the list of all the strings in the vocab

# creating a index-key dictionary for our vocabulary
stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})
len(itos)

# creating a index representation for our train and validation dataset
trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])
val_lm = np.array([[stoi[o] for o in p] for p in tok_val])

# saving our indexed representation of our dataset to disk
# we also save the index-word mapping to retrieve the complete text representation from these numpy arrays
np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)
np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)
pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))

# Loading the indexed representation of our dataset from disk
# we also load the index-word mapping to to help us convert the indexes to word datasets, if need be.
trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')
val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy')
itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))

# checking vocabulary size
vs=len(itos)
vs,len(trn_lm)

bs = 32

trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)
val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)
md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)

drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7

learner= md.get_model(opt_fn, em_sz, nh, nl, 
    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])

learner.metrics = [accuracy]
learner.unfreeze()

lr=1e-3
lrs = lr

learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)

learner.sched.plot_loss()

learner.save('lm_PT_v2_fastai')

# Learning rate checking
learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)

learner.sched.plot()

"""### Dropout factor = 0.05, learning rate = 1e-3"""

drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.05

learner= md.get_model(opt_fn, em_sz, nh, nl, 
    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])

learner.metrics = [accuracy]
learner.unfreeze()

lr=1e-3
lrs = lr

learner.fit(lrs, 1, wds=wd, use_clr=(32,2), cycle_len=1)

learner.sched.plot_loss()

learner.save('lm_PT_v2_fastai_dp_005')

# Learning rate checking
learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)

learner.sched.plot()

"""### Reduced vocabulary size (30000 tokens), batch size = 52"""

# Truncating our vocab to ignore the rare words
max_vocab = 30000
min_freq = 5

itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq] # getting rid of the rare words
itos.insert(0, '_pad_') # 
itos.insert(0, '_unk_') # itos is the list of all the strings in the vocab

# creating a index-key dictionary for our vocabulary
stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})
len(itos)

# creating a index representation for our train and validation dataset
trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])
val_lm = np.array([[stoi[o] for o in p] for p in tok_val])

# saving our indexed representation of our dataset to disk
# we also save the index-word mapping to retrieve the complete text representation from these numpy arrays
np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)
np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)
pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))

# Loading the indexed representation of our dataset from disk
# we also load the index-word mapping to to help us convert the indexes to word datasets, if need be.
trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')
val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy')
itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))

# checking vocabulary size
vs=len(itos)
vs,len(trn_lm)

bs = 52

trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)
val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)
md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)

drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.05

learner= md.get_model(opt_fn, em_sz, nh, nl, 
    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])

# Learning rate checking
learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)

learner.sched.plot()

learner.fit(lrs, 1, wds=wd, use_clr=(32,2), cycle_len=1)

learner.sched.plot_loss()

learner.save('lm_PT_v2_fastai_dp_005_30k_tokens_bs52')

"""### 1-cycle"""

# Truncating our vocab to ignore the rare words
max_vocab = 30000
min_freq = 5

itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq] # getting rid of the rare words
itos.insert(0, '_pad_') # 
itos.insert(0, '_unk_') # itos is the list of all the strings in the vocab

# creating a index-key dictionary for our vocabulary
stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})
len(itos)

# creating a index representation for our train and validation dataset
trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])
val_lm = np.array([[stoi[o] for o in p] for p in tok_val])

# saving our indexed representation of our dataset to disk
# we also save the index-word mapping to retrieve the complete text representation from these numpy arrays
np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)
np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)
pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))

# Loading the indexed representation of our dataset from disk
# we also load the index-word mapping to to help us convert the indexes to word datasets, if need be.
trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')
val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy')
itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))

# checking vocabulary size
vs=len(itos)
vs,len(trn_lm)

bs = 52

trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)
val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)
md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)

opt_fn = partial(optim.SGD, momentum=0.9)

"""#### Dropout constant = 0.05, no gradient clipping"""

drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.05 #the higher the lr, the lower dp

learner= md.get_model(opt_fn, em_sz, nh, nl, 
    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])

learner.metrics = [accuracy]
learner.unfreeze()

learner.lr_find2(num_it=200)

learner.sched.plot()

lr = 5

learner.fit(lr, 1, cycle_len=2, use_clr_beta=(10,10,0.95,0.85))

learner.sched.plot_loss()

learner.save('lm_PT_1_cycle')

"""#### Dropout constant = 0.1, gradient clipping = 0.25"""

drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.1

learner= md.get_model(opt_fn, em_sz, nh, nl, 
    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])

learner.metrics = [accuracy]
learner.clip=0.25
learner.unfreeze()

learner.clip

learner.lr_find2(num_it=500)

learner.sched.plot()

lr = 6

learner.fit(lr, 1, cycle_len=2, use_clr_beta=(10,10,0.95,0.85))

learner.sched.plot_loss()

learner.save('lm_PT_1_cycle_dp_gc')

"""## Language Model Test"""

learner.load('lm_PT_1_cycle_10_epochs_from_scratch')

m = learner.model
m.eval()
m[0].bs = 1

sen = "o universo é"

idxs = np.array([[stoi[p] for p in sen.strip().split(" ")]])
idxs

def get_next(inp):
    m[0].bs =1
    idxs = np.array([[stoi[p] for p in inp.strip().split(" ")]])
    probs = m(VV(idxs))
    encc = probs[-1][-1][-1][-1].squeeze()
    p = learner.model[1].decoder(encc).exp()
    ret = to_np(torch.multinomial(p, 1)[0])
        
    while ret == 0:
        ret = to_np(torch.multinomial(learner.model[1].decoder(encc).exp(), 1)[0])
    
    try:
        r = itos[ret[0]]
    except:
        r = "oor"
    return r


def get_next_n(inp, n):
    res = inp
    for i in range(n):
        c = get_next(inp)
        res = res + " " + c
        inp = inp.strip().split(" ") + [c]    
        inp = ' '.join(inp)
        
    return res

def get_next2(inp):
    m[0].bs =1
    idxs = np.array([[stoi[p] for p in inp.strip().split(" ")]])
    probs = m(VV(idxs))
    encc = probs[-1][-1][-1][-1].squeeze()
    pred = to_np(learner.model[1].decoder(encc).exp()).argmax()
        
    if pred == 0:
        pred = to_np(learner.model[1].decoder(encc).exp()).argsort()[-2]
    
    try:
        r = itos[pred]
    except:
        r = "oor"
    return r


def get_next_n2(inp, n):
    res = inp
    for i in range(n):
        c = get_next2(inp)
        res = res + " " + c
        inp = inp.strip().split(" ") + [c]    
        inp = ' '.join(inp)
        
    return res

print(get_next(sen))

print(get_next_n(sen, 70))

print(get_next_n("o brasil", 70))

print(get_next_n("a vida", 70))

print(get_next_n("o tribunal de contas", 70))

print(get_next_n2("o universo é", 2))

print(get_next_n2("o brasil", 2))

print(get_next_n2("a vida", 2))

print(get_next_n2("o tribunal de contas", 2))

learner.summary



